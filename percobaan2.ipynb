{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library yang kalian butuhkan\n",
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder_path):\n",
    "    data = []\n",
    "    file_names = []\n",
    "    \n",
    "    try:\n",
    "        dataset = os.listdir(folder_path)\n",
    "        \n",
    "        for file in dataset:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            name, ext = os.path.splitext(file)\n",
    "            \n",
    "            try:\n",
    "                img = cv.imread(file_path)\n",
    "                if img is None:\n",
    "                    raise Exception(\"Failed to read image\")\n",
    "                \n",
    "                data.append(img.astype(np.uint8))\n",
    "                file_names.append(name)\n",
    "            except cv.error as e:\n",
    "                print(f\"Error reading image {file}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {file}: {e}\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Folder not found: {folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing folder {folder_path}: {e}\")\n",
    "    \n",
    "    return data, file_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_image(data, file_name, path):\n",
    "    for i in range(len(data)):\n",
    "        image = data[i].astype(np.uint8)\n",
    "        min_val = np.min(image)\n",
    "        max_val = np.max(image)\n",
    "        \n",
    "        new_image = (image - min_val) / (max_val - min_val)\n",
    "        int_image = (new_image * 255).astype(np.uint8)\n",
    "        \n",
    "        resized_image = cv.resize(int_image, (300, 300), interpolation=cv.INTER_NEAREST)\n",
    "        data[i] = resized_image\n",
    "        \n",
    "        save_image_path = path + file_name[i] + \".jpg\"\n",
    "        cv.imwrite (save_image_path, data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing image 05_F1D022161_1_001.Jpg: Failed to read image\n",
      "Error processing image 05_F1D022161_1_002.Jpg: Failed to read image\n",
      "Error processing image 05_F1D022161_1_003.Jpg: Failed to read image\n",
      "Error processing image 08_F1D022052_1_1.jpg: Failed to read image\n",
      "Error processing image 08_F1D022052_1_2.jpg: Failed to read image\n",
      "Error processing image 08_F1D022052_1_3.jpg: Failed to read image\n",
      "Error processing image 08_F1D022157_1_1.JPG: Failed to read image\n",
      "Error processing image 08_F1D022157_1_2.JPG: Failed to read image\n",
      "Error processing image 08_F1D022157_1_3.JPG: Failed to read image\n",
      "Error processing image 10_F1D022151_1_1.jpg: Failed to read image\n",
      "Error processing image 10_F1D022151_1_2.jpg: Failed to read image\n",
      "Error processing image 10_F1D022151_1_3.jpg: Failed to read image\n",
      "Error processing image 17_F1D022106_1_1.Jpg: Failed to read image\n",
      "Error processing image 17_F1D022106_1_2.Jpg: Failed to read image\n",
      "Error processing image 17_F1D022106_1_3.Jpg: Failed to read image\n",
      "Error processing image 18_F1D022046_1_1.jpg: Failed to read image\n",
      "Error processing image 18_F1D022046_1_2.jpg: Failed to read image\n",
      "Error processing image 18_F1D022046_1_3.jpg: Failed to read image\n",
      "Error processing image 05_F1D022161_2_001.Jpg: Failed to read image\n",
      "Error processing image 05_F1D022161_2_002.Jpg: Failed to read image\n",
      "Error processing image 05_F1D022161_2_003.Jpg: Failed to read image\n",
      "Error processing image 08_F1D022052_2_1.jpg: Failed to read image\n",
      "Error processing image 08_F1D022052_2_2.jpg: Failed to read image\n",
      "Error processing image 08_F1D022052_2_3.jpg: Failed to read image\n",
      "Error processing image 08_F1D022157_2_1.JPG: Failed to read image\n",
      "Error processing image 08_F1D022157_2_2.JPG: Failed to read image\n",
      "Error processing image 08_F1D022157_2_3.JPG: Failed to read image\n",
      "Error processing image 10_F1D022151_2_1.jpg: Failed to read image\n",
      "Error processing image 10_F1D022151_2_2.jpg: Failed to read image\n",
      "Error processing image 10_F1D022151_2_3.jpg: Failed to read image\n",
      "Error processing image 17_F1D022106_2_1.Jpg: Failed to read image\n",
      "Error processing image 17_F1D022106_2_2.Jpg: Failed to read image\n",
      "Error processing image 17_F1D022106_2_3.Jpg: Failed to read image\n",
      "Error processing image 18_F1D022046_2_1.jpg: Failed to read image\n",
      "Error processing image 18_F1D022046_2_2.jpg: Failed to read image\n",
      "Error processing image 18_F1D022046_2_3.jpg: Failed to read image\n"
     ]
    }
   ],
   "source": [
    "images, file_names = load_images_from_folder('dataset/finger_1/')\n",
    "write_image(images, file_names, 'output/finger_1/')\n",
    "images, file_names = load_images_from_folder('dataset/finger_2/')\n",
    "write_image(images,file_names, 'resizeImage/finger_2/')\n",
    "images, file_names = load_images_from_folder('dataset/finger_3/')\n",
    "write_image(images,file_names, 'output/finger_3')\n",
    "images, file_names = load_images_from_folder('dataset/finger_4/')\n",
    "write_image(images,file_names, 'output/finger_4/')\n",
    "images, file_names = load_images_from_folder('dataset/finger_5/')\n",
    "write_image(images,file_names, 'output/finger_5/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m             output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, output_filename)\n\u001b[0;32m     29\u001b[0m             cv\u001b[38;5;241m.\u001b[39mimwrite(output_path, rotated_image)\n\u001b[1;32m---> 31\u001b[0m \u001b[43maugment_images\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresizeImage/finger_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maugment/finger_1/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m augment_images(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresizeImage/finger_2\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maugment/finger_2/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m augment_images(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresizeImage/finger_3\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maugment/finger_3/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m, in \u001b[0;36maugment_images\u001b[1;34m(image_dir, output_dir)\u001b[0m\n\u001b[0;32m     24\u001b[0m image \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m angle \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m120\u001b[39m, \u001b[38;5;241m360\u001b[39m]:\n\u001b[1;32m---> 26\u001b[0m     rotated_image \u001b[38;5;241m=\u001b[39m \u001b[43mrotate_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     output_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_rotated_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mangle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     28\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, output_filename)\n",
      "Cell \u001b[1;32mIn[5], line -1\u001b[0m, in \u001b[0;36mrotate_image\u001b[1;34m(image, angle)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def rotate_image(image, angle):\n",
    "    height, width = image.shape[:2]\n",
    "    center_x, center_y = width // 2, height // 2\n",
    "\n",
    "    radians = np.deg2rad(angle)\n",
    "    cos_val = np.cos(radians)\n",
    "    sin_val = np.sin(radians)\n",
    "\n",
    "    rotated_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "    for y in range(height):\n",
    "        for x in range(width):\n",
    "            xp = int(round(cos_val * (x - center_x) - sin_val * (y - center_y) + center_x))\n",
    "            yp = int(round(sin_val * (x - center_x) + cos_val * (y - center_y) + center_y))\n",
    "\n",
    "            if 0 <= xp < width and 0 <= yp < height:\n",
    "                rotated_image[y, x] = image[yp, xp]\n",
    "\n",
    "    return rotated_image\n",
    "\n",
    "def augment_images(image_dir, output_dir):\n",
    "    for filename in os.listdir(image_dir):\n",
    "        image_path = os.path.join(image_dir, filename)\n",
    "        image = cv.imread(image_path)\n",
    "        for angle in [30, 90, 120, 360]:\n",
    "            rotated_image = rotate_image(image, angle)\n",
    "            output_filename = f'{filename[:-4]}_rotated_{angle}.jpg'\n",
    "            output_path = os.path.join(output_dir, output_filename)\n",
    "            cv.imwrite(output_path, rotated_image)\n",
    "            \n",
    "augment_images('resizeImage/finger_1','augment/finger_1/')\n",
    "augment_images('resizeImage/finger_2','augment/finger_2/')\n",
    "augment_images('resizeImage/finger_3','augment/finger_3/')\n",
    "augment_images('resizeImage/finger_4','augment/finger_4/')\n",
    "augment_images('resizeImage/finger_5','augment/finger_5/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images_in_folder(folder_path, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = cv.imread(img_path)\n",
    "            gray_img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "            normalized_img = normalize(gray_img)\n",
    "            output_path = os.path.join(output_folder, f\"normalized_{filename}\")\n",
    "            cv.imwrite(output_path, normalized_img)\n",
    "\n",
    "def normalize(citra):\n",
    "    min_val = np.min(citra)\n",
    "    max_val = np.max(citra)\n",
    "    normalized_citra = (citra - min_val) / (max_val - min_val) * 255\n",
    "    return normalized_citra.astype(np.uint8)\n",
    "\n",
    "normalize_images_in_folder('augment/finger_1', 'normalized_images/finger_1')\n",
    "normalize_images_in_folder('augment/finger_2', 'normalized_images/finger_2')\n",
    "normalize_images_in_folder('augment/finger_3', 'normalized_images/finger_3')\n",
    "normalize_images_in_folder('augment/finger_4', 'normalized_images/finger_4')\n",
    "normalize_images_in_folder('augment/finger_5', 'normalized_images/finger_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurence(citra, rentang, derajat, distance=1):\n",
    "    matrixCO = np.zeros([rentang+1,rentang+1])\n",
    "    p,l = citra.shape\n",
    "    \n",
    "    if(derajat == 0):\n",
    "        for i in range(p):\n",
    "            for j in range(l):\n",
    "                if(j+distance < l):\n",
    "                    matrixCO[citra[i,j], citra[i,j+distance]] += 1\n",
    "    \n",
    "    elif (derajat == 45):\n",
    "        for i in range(p):\n",
    "            for j in range(l):\n",
    "                if(i-distance >= 0 and j+distance < l):\n",
    "                    matrixCO[citra[i,j], citra[i-distance,j+distance]] += 1\n",
    "                \n",
    "    elif (derajat == 90):\n",
    "        for i in range(p):\n",
    "            for j in range(l):\n",
    "                if(i-distance >= 0):\n",
    "                    matrixCO[citra[i,j], citra[i-distance,j]] += 1\n",
    "    \n",
    "    elif (derajat == 135):\n",
    "        for i in range(p):\n",
    "            for j in range(l):\n",
    "                if(i-distance >=0 and j-distance >=0 ):\n",
    "                    matrixCO[citra[i,j], citra[i-distance,j-distance]] += 1\n",
    "                \n",
    "    return matrixCO\n",
    "\n",
    "def getGLCM(citra, derajat, distance=1):\n",
    "    co = co_occurence(citra, 255 ,derajat, distance)\n",
    "    simetris = co + np.transpose(co)\n",
    "    normal = simetris/np.sum(simetris)\n",
    "    return normal\n",
    "\n",
    "def countFeatures(array):\n",
    "    p,l = array.shape\n",
    "\n",
    "    kontras = 0\n",
    "    diss = 0\n",
    "    hg = 0\n",
    "    ent = 0\n",
    "    asm = 0\n",
    "\n",
    "    miux = 0\n",
    "    miuy = 0 \n",
    "    sigx = 0 \n",
    "    sigy = 0\n",
    "    corr = 0\n",
    "\n",
    "    for i in range (p):\n",
    "        for j in range (l):\n",
    "            kontras += array[i][j] * (pow((i-j),2))\n",
    "            diss += array[i][j] * (abs(i-j))\n",
    "            hg += array[i][j] / (1+((i-j)*(i-j)))\n",
    "            if(array[i][j]!=0):\n",
    "                ent += -(array[i][j] * (math.log(array[i][j],2)))\n",
    "\n",
    "            asm += pow(array[i][j],2)\n",
    "            \n",
    "            miux += i * array[i,j]\n",
    "            miuy += j * array[i,j]\n",
    "\n",
    "    for i in range (p):\n",
    "        for j in range (l):\n",
    "            sigx += pow((1-miux), 2) * array[i,j]\n",
    "            sigy += pow((1-miuy), 2) * array[i,j]\n",
    "    \n",
    "    for i in range (p):\n",
    "        for j in range(l):\n",
    "            corr += ((i-miux)*(j-miuy)*array[i,j]) / (math.sqrt(sigx*sigy))\n",
    "\n",
    "    eng = math.sqrt(asm)\n",
    "\n",
    "    return [kontras, diss, hg, ent, asm, eng, corr]\n",
    "\n",
    "def ekstraksiFitur(matriks):\n",
    "    baris = countFeatures(matriks)\n",
    "    return baris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ekstraksiData(names, images, labels, distance=1):\n",
    "    features = []\n",
    "    count = len(images)\n",
    "    for name, image,label in zip(names, images, labels):\n",
    "        print(f\"process : [{label}] {name}\")\n",
    "\n",
    "        glcm0 = getGLCM(image, 0, distance)\n",
    "        glcm45 = getGLCM(image, 45, distance)\n",
    "        glcm90 = getGLCM(image, 90, distance)\n",
    "        glcm135 = getGLCM(image, 135, distance)  \n",
    "\n",
    "        fitur0 = ekstraksiFitur(glcm0)\n",
    "        fitur45 = ekstraksiFitur(glcm45)\n",
    "        fitur90 = ekstraksiFitur(glcm90)\n",
    "        fitur135 = ekstraksiFitur(glcm135)\n",
    "        fitur = [\n",
    "            fitur0[0], fitur0[1], fitur0[2], fitur0[3], fitur0[4], fitur0[5], fitur0[6], \n",
    "            fitur45[0], fitur45[1], fitur45[2], fitur45[3], fitur45[4], fitur45[5], fitur45[6], \n",
    "            fitur90[0],fitur90[1],fitur90[2],fitur90[3], fitur90[4], fitur90[5], fitur90[6], \n",
    "            fitur135[0],fitur135[1],fitur135[2],fitur135[3], fitur135[4], fitur135[5], fitur135[6],\n",
    "            label\n",
    "        ]\n",
    "        # print(fitur)\n",
    "        features.append(fitur)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"normalize_images/\" \n",
    "\n",
    "imgs = [] \n",
    "labels = []\n",
    "names = []\n",
    "for sub_folder in os.listdir(dataset_dir):\n",
    "    sub_folder_files = os.listdir(os.path.join(dataset_dir, sub_folder))\n",
    "    for i, filename in enumerate(sub_folder_files[:100]):\n",
    "        img = cv.imread(os.path.join(dataset_dir, sub_folder, filename))\n",
    "        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        imgs.append(gray)\n",
    "        labels.append(sub_folder)\n",
    "        names.append(filename)\n",
    "\n",
    "imgs = np.array(imgs)\n",
    "labels = np.array(labels)\n",
    "names = np.array(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ekstraksiData(names, imgs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"Kontras0\",\n",
    "    \"Dissimilarity0\",\n",
    "    \"Homogenitas0\",\n",
    "    \"Entropi0\",\n",
    "    \"ASM0\",\n",
    "    \"Energy0\",\n",
    "    \"Correlation0\",\n",
    "    \"Kontras45\",\n",
    "    \"Dissimilarity45\",\n",
    "    \"Homogenitas45\",\n",
    "    \"Entropi45\",\n",
    "    \"ASM45\",\n",
    "    \"Energy45\",\n",
    "    \"Correlation45\",\n",
    "    \"Kontras90\",\n",
    "    \"Dissimilarity90\",\n",
    "    \"Homogenitas90\",\n",
    "    \"Entropi90\",\n",
    "    \"ASM90\",\n",
    "    \"Energy90\",\n",
    "    \"Correlation90\",\n",
    "    \"Kontras135\",\n",
    "    \"Dissimilarity135\",\n",
    "    \"Homogenitas135\",\n",
    "    \"Entropi135\",\n",
    "    \"ASM135\",\n",
    "    \"Energy135\",\n",
    "    \"Correlation135\",\n",
    "    \"label\",\n",
    "]\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "glcm_df = pd.DataFrame(features, \n",
    "                      columns = columns)\n",
    "glcm_df.info()\n",
    "glcm_df.to_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)  \n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return headers, data\n",
    "\n",
    "\n",
    "def modify_labels(data, label_column_index):\n",
    "    label_mapping = {'finger_1': 1, 'finger_2': 2, 'finger_3': 3, 'finger_4': 4, 'finger_5': 5}\n",
    "    for row in data:\n",
    "        row[label_column_index] = label_mapping.get(row[label_column_index], row[label_column_index])\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_correlation_matrix(data):\n",
    "    data = np.array(data, dtype=float)\n",
    "    n = data.shape[1]\n",
    "    correlation_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            correlation_matrix[i, j] = calculate_correlation(data[:, i], data[:, j])\n",
    "    return correlation_matrix\n",
    "\n",
    "\n",
    "def calculate_correlation(x, y):\n",
    "    mean_x, mean_y = np.mean(x), np.mean(y)\n",
    "    std_x, std_y = np.std(x), np.std(y)\n",
    "    covariance = np.mean((x - mean_x) * (y - mean_y))\n",
    "    return covariance / (std_x * std_y)\n",
    "\n",
    "\n",
    "def plot_correlation_matrix(correlation_matrix, headers):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(correlation_matrix, cmap='coolwarm', interpolation='none')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(headers)), headers, rotation=90)\n",
    "    plt.yticks(range(len(headers)), headers)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def remove_highly_correlated_features(correlation_matrix, headers, threshold=0.9):\n",
    "    to_drop = set()\n",
    "    n = correlation_matrix.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if abs(correlation_matrix[i, j]) > threshold:\n",
    "                to_drop.add(headers[j])\n",
    "    reduced_headers = [header for header in headers if header not in to_drop]\n",
    "    return reduced_headers\n",
    "\n",
    "\n",
    "def save_reduced_dataset(file_path, headers, data):\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(data)\n",
    "\n",
    "file_path = 'dataset.csv'  \n",
    "headers, data = read_csv(file_path)\n",
    "\n",
    "label_column_index = -1  \n",
    "data = modify_labels(data, label_column_index)\n",
    "\n",
    "data = np.array(data, dtype=float)\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "\n",
    "correlation_matrix = calculate_correlation_matrix(features)\n",
    "\n",
    "plot_correlation_matrix(correlation_matrix, headers[:-1])\n",
    "\n",
    "reduced_headers = remove_highly_correlated_features(correlation_matrix, headers[:-1], threshold=0.9)\n",
    "\n",
    "reduced_headers.append(headers[label_column_index])\n",
    "reduced_features = features[:, [headers.index(h) for h in reduced_headers[:-1]]]\n",
    "reduced_data = np.hstack((reduced_features, labels.reshape(-1, 1)))\n",
    "\n",
    "save_reduced_dataset('reduced_dataset.csv', reduced_headers, reduced_data)\n",
    "\n",
    "print(f\"Original number of features: {len(headers) - 1}\")\n",
    "print(f\"Reduced number of features: {len(reduced_headers) - 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)  \n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return headers, data\n",
    "\n",
    "def split_data(data, test_ratio=0.3):\n",
    "    np.random.shuffle(data)  \n",
    "    test_size = int(len(data) * test_ratio)\n",
    "    test_data = data[:test_size]\n",
    "    train_data = data[test_size:]\n",
    "    return train_data, test_data\n",
    "\n",
    "def save_dataset(file_path, headers, data):\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(data)\n",
    "\n",
    "file_path = 'reduced_dataset.csv'  \n",
    "headers, data = read_csv(file_path)\n",
    "\n",
    "train_data, test_data = split_data(data, test_ratio=0.3)\n",
    "\n",
    "save_dataset('reduced_train_glcm_dataset.csv', headers, train_data)\n",
    "save_dataset('reduced_test_glcm_dataset.csv', headers, test_data)\n",
    "\n",
    "print(f\"Training set size: {len(train_data)}\")\n",
    "print(f\"Testing set size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader) \n",
    "        for row in reader:\n",
    "            data.append([float(value) for value in row])\n",
    "    return headers, np.array(data)\n",
    "\n",
    "def min_max_normalize(data):\n",
    "    min_vals = np.min(data, axis=0)\n",
    "    max_vals = np.max(data, axis=0)\n",
    "    norm_data = (data - min_vals) / (max_vals - min_vals)\n",
    "    return norm_data\n",
    "\n",
    "def save_dataset(file_path, headers, data):\n",
    "    with open(file_path, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(headers)\n",
    "        writer.writerows(data)\n",
    "\n",
    "file_path = 'reduced_train_glcm_dataset.csv' \n",
    "headers, data = read_csv(file_path)\n",
    "\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "\n",
    "norm_features = min_max_normalize(features)\n",
    "\n",
    "normalized_data = np.hstack((norm_features, labels.reshape(-1, 1)))\n",
    "\n",
    "save_dataset('normalized_train_glcm_dataset.csv', headers, normalized_data)\n",
    "\n",
    "print(\"Data normalization completed and saved to 'normalized_train_glcm_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'reduced_test_glcm_dataset.csv' \n",
    "headers, data = read_csv(file_path)\n",
    "\n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "\n",
    "norm_features = min_max_normalize(features)\n",
    "\n",
    "normalized_data = np.hstack((norm_features, labels.reshape(-1, 1)))\n",
    "\n",
    "save_dataset('normalized_test_glcm_dataset.csv', headers, normalized_data)\n",
    "\n",
    "print(\"Data normalization completed and saved to 'normalized_test_glcm_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def read_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)  \n",
    "        for row in reader:\n",
    "            data.append([float(value) for value in row])\n",
    "    return headers, np.array(data)\n",
    "\n",
    "\n",
    "def min_max_normalize(data):\n",
    "    min_vals = np.min(data, axis=0)\n",
    "    max_vals = np.max(data, axis=0)\n",
    "    norm_data = (data - min_vals) / (max_vals - min_vals)\n",
    "    return norm_data\n",
    "\n",
    "\n",
    "train_file_path = 'normalized_train_glcm_dataset.csv'  \n",
    "train_headers, train_data = read_csv(train_file_path)\n",
    "train_features = train_data[:, :-1]\n",
    "train_labels = train_data[:, -1]\n",
    "train_features = min_max_normalize(train_features)\n",
    "\n",
    "\n",
    "test_file_path = 'normalized_test_glcm_dataset.csv'  \n",
    "test_headers, test_data = read_csv(test_file_path)\n",
    "test_features = test_data[:, :-1]\n",
    "test_labels = test_data[:, -1]\n",
    "test_features = min_max_normalize(test_features)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "svm = SVC(kernel='linear')\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "\n",
    "knn.fit(train_features, train_labels)\n",
    "svm.fit(train_features, train_labels)\n",
    "rf.fit(train_features, train_labels)\n",
    "\n",
    "\n",
    "knn_predictions = knn.predict(test_features)\n",
    "svm_predictions = svm.predict(test_features)\n",
    "rf_predictions = rf.predict(test_features)\n",
    "\n",
    "\n",
    "knn_accuracy = accuracy_score(test_labels, knn_predictions)\n",
    "svm_accuracy = accuracy_score(test_labels, svm_predictions)\n",
    "rf_accuracy = accuracy_score(test_labels, rf_predictions)\n",
    "\n",
    "\n",
    "print(f\"KNN Accuracy: {knn_accuracy * 100:.2f}%\")\n",
    "print(f\"SVM Accuracy: {svm_accuracy * 100:.2f}%\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Hitung metrik evaluasi tambahan: presisi, recall, dan F1-score\n",
    "knn_precision = precision_score(test_labels, knn_predictions, average='weighted')\n",
    "knn_recall = recall_score(test_labels, knn_predictions, average='weighted')\n",
    "knn_f1_score = f1_score(test_labels, knn_predictions, average='weighted')\n",
    "\n",
    "svm_precision = precision_score(test_labels, svm_predictions, average='weighted')\n",
    "svm_recall = recall_score(test_labels, svm_predictions, average='weighted')\n",
    "svm_f1_score = f1_score(test_labels, svm_predictions, average='weighted')\n",
    "\n",
    "rf_precision = precision_score(test_labels, rf_predictions, average='weighted')\n",
    "rf_recall = recall_score(test_labels, rf_predictions, average='weighted')\n",
    "rf_f1_score = f1_score(test_labels, rf_predictions, average='weighted')\n",
    "\n",
    "results = {\n",
    "    'Model': ['KNN', 'SVM', 'Random Forest'],\n",
    "    'Accuracy': [knn_accuracy, svm_accuracy, rf_accuracy],\n",
    "    'Precision': [knn_precision, svm_precision, rf_precision],\n",
    "    'Recall': [knn_recall, svm_recall, rf_recall],\n",
    "    'F1-Score': [knn_f1_score, svm_f1_score, rf_f1_score]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_report(report, title='Classification Report', cmap='coolwarm'):\n",
    "    lines = report.split('\\n')\n",
    "    \n",
    "    classes = []\n",
    "    matrix = []\n",
    "    \n",
    "    for line in lines[2:(len(lines) - 3)]:\n",
    "        row = line.strip().split()\n",
    "        if len(row) > 0:\n",
    "            # Skip rows that do not contain class metrics\n",
    "            if len(row) == 5:\n",
    "                classes.append(row[0])\n",
    "                matrix.append([float(x) for x in row[1:-1]])\n",
    "\n",
    "    matrix = np.array(matrix)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, len(classes) * 1))\n",
    "    cax = ax.matshow(matrix, cmap=cmap)\n",
    "    plt.title(title, pad=20)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(matrix[0])))\n",
    "    ax.set_yticks(np.arange(len(classes)))\n",
    "\n",
    "    ax.set_xticklabels(['Precision', 'Recall', 'F1-Score'], rotation=45)\n",
    "    ax.set_yticklabels(classes)\n",
    "    \n",
    "    for i in range(len(classes)):\n",
    "        for j in range(len(matrix[i])):\n",
    "            ax.text(j, i, f'{matrix[i, j]:.2f}', va='center', ha='center', color='black')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Classes')\n",
    "    plt.show()\n",
    "\n",
    "classification_report_knn = classification_report(test_labels, knn_predictions)\n",
    "plot_classification_report(classification_report_knn, title='Classification Report KNN')\n",
    "\n",
    "classification_report_svm = classification_report(test_labels, svm_predictions)\n",
    "plot_classification_report(classification_report_svm, title='Classification Report SVM')\n",
    "\n",
    "classification_report_rf = classification_report(test_labels, rf_predictions)\n",
    "plot_classification_report(classification_report_rf, title='Classification Report Random Forest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(test_labels, knn_predictions, title='KNN Confusion Matrix')\n",
    "\n",
    "plot_confusion_matrix(test_labels, svm_predictions, title='SVM Confusion Matrix')\n",
    "\n",
    "plot_confusion_matrix(test_labels, rf_predictions, title='RF Confusion Matrix')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
